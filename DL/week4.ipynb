{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_wt = [1,0.9,0.6,0.01,0.1,0.2,0.5,0.55,0.56]\n",
    "v_1 = 0\n",
    "epsilon = 0\n",
    "beta = 0.9\n",
    "initial_learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wt0:  0.316227766016838\n",
      "wt1:  0.21764287503300353\n",
      "wt2:  0.13768567816430285\n",
      "wt3:  0.002418820039082076\n",
      "wt4:  0.025414130057157588\n",
      "wt5:  0.052824867720968664\n",
      "wt6:  0.1274073800437306\n",
      "wt7:  0.13384427388811823\n",
      "wt8:  0.13078770938080816\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.13078770938080816)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def adagrad(delta_wt, v_1, epsilon, beta, initial_learning_rate):\n",
    "    # calculate the learning rate for each weight\n",
    "    v_1 = 0\n",
    "    for i in range(len(delta_wt)):\n",
    "        v_1 = beta*v_1 + (1-beta)*(delta_wt[i]**2)\n",
    "        wt = initial_learning_rate/(np.sqrt(v_1)+epsilon)*delta_wt[i]\n",
    "        print(f\"wt{i}: \", wt)\n",
    "    return wt\n",
    "\n",
    "adagrad(delta_wt, v_1, epsilon, beta, initial_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wt0:  0.316227766016838\n",
      "wt1:  0.21764287503300353\n",
      "wt2:  0.13768567816430285\n",
      "wt3:  0.002418820039082076\n",
      "wt4:  0.025414130057157588\n",
      "wt5:  0.052824867720968664\n",
      "wt6:  0.1274073800437306\n",
      "wt7:  0.13384427388811823\n",
      "wt8:  0.13078770938080816\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.13078770938080816)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rmsprop(delta_wt, v_1, epsilon, beta, initial_learning_rate):\n",
    "    # calculate the learning rate for each weight\n",
    "    v_1 = 0\n",
    "    for i in range(len(delta_wt)):\n",
    "        v_1 = beta*v_1 + (1-beta)*(delta_wt[i]**2)\n",
    "        wt = initial_learning_rate/(np.sqrt(v_1)+epsilon)*delta_wt[i]\n",
    "        print(f\"wt{i}: \", wt)\n",
    "    return wt\n",
    "\n",
    "rmsprop(delta_wt, v_1, epsilon, beta, initial_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wt0:  0.1\n",
      "wt1:  0.09870904159445826\n",
      "wt2:  0.08983536339720438\n",
      "wt3:  0.04884987294533668\n",
      "wt4:  0.0292365520037817\n",
      "wt5:  0.021228431510008902\n",
      "wt6:  0.029553126373334942\n",
      "wt7:  0.03791722196700382\n",
      "wt8:  0.04471301218359424\n",
      "wt9:  0.039905338144962034\n",
      "-------------------\n",
      "Loss is:  [0.6525961997180987, 0.5094763074316707, 0.2042161312707228, -0.3282970425221387, -0.19832502283968922, -0.07445168947469231, 0.21466926273091635, 0.25019564789420085, 0.24683310307689488]\n",
      "Loss is:  [0.6375464933083825, 0.4908368085886095, 0.18539842099926052, -0.3419500113412334, -0.20862944454847754, -0.08298115211785488, 0.2048460537247609, 0.23885280330971892, 0.2341562119200922]\n",
      "Loss is:  [0.6197618263341763, 0.4699222402259378, 0.16472132526286337, -0.3571398973905785, -0.22021404754740334, -0.09257748329570956, 0.19416089512744406, 0.2268476325390366, 0.22100999610727717]\n",
      "Loss is:  [0.601732613839301, 0.4489967265782907, 0.1442411497273523, -0.37200065639863267, -0.23138659017917806, -0.1017069968313497, 0.18400272399409348, 0.2154368021936307, 0.2085301488353391]\n",
      "Loss is:  [0.5844834251561637, 0.42898248949768425, 0.12478592485753298, -0.38579035221748376, -0.24148345051435224, -0.10977710116469297, 0.1748996028541436, 0.2050896251457395, 0.19713238800388364]\n",
      "Loss is:  [0.5683806061562922, 0.41020012750282303, 0.10663393981358253, -0.39826923523486435, -0.25029961072485124, -0.11661387434249514, 0.16699751096558021, 0.19592684619457507, 0.1869145630677968]\n",
      "Loss is:  [0.553500757652251, 0.39270626257415786, 0.08982312197964804, -0.40941627893305677, -0.2578289539459759, -0.12222414449808237, 0.1602782927158959, 0.18792028130791344, 0.17783950903912338]\n",
      "Loss is:  [0.5397982014499224, 0.37644689936632053, 0.07429146387279796, -0.41930060843989125, -0.2641465609626672, -0.12668774936812371, 0.1546582654056997, 0.18098305391565775, 0.16981760929570203]\n",
      "Loss is:  [0.527180509666344, 0.36132650776743674, 0.05994019679507456, -0.4280237427254434, -0.26935591621003746, -0.1301092915970757, 0.1500322947455859, 0.17500984978239875, 0.16274354948281192]\n",
      "Loss is:  [0.5155420507380795, 0.3472386296543243, 0.04666170452532261, -0.43569414834096576, -0.27356572210190716, -0.1325970239837877, 0.14629301265008926, 0.16989439739862777, 0.15651220684258993]\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "def adam(delta_wt, epsilon, beta1, beta2, initial_learning_rate):\n",
    "    m = 0\n",
    "    v = 0\n",
    "    t = 0\n",
    "    # Run for 10 iterations\n",
    "    for i in range(10):\n",
    "        t += 1\n",
    "        # Use available gradient or default to 0 if not enough data\n",
    "        g = delta_wt[i] if i < len(delta_wt) else 0\n",
    "        m = beta1 * m + (1 - beta1) * g\n",
    "        v = beta2 * v + (1 - beta2) * (g ** 2)\n",
    "        m_hat = m / (1 - beta1**t)\n",
    "        v_hat = v / (1 - beta2**t)\n",
    "        wt = initial_learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "        print(f\"wt{i}: \", wt)\n",
    "    return wt\n",
    "\n",
    "def momentum_update(parameters, gradients, learning_rate, beta, previous_momentum):\n",
    "\n",
    "    new_momentum = beta * previous_momentum + (1 - beta) * gradients\n",
    "\n",
    "    new_parameters = parameters - learning_rate * new_momentum\n",
    "\n",
    "    return new_parameters, new_momentum\n",
    "\n",
    "\n",
    "def momentum(delta_wt, beta, initial_learning_rate, epochs):\n",
    "    # Initialize the momentum\n",
    "    momentum = 0\n",
    "    for i in range(epochs):\n",
    "        for j in range(len(delta_wt)):\n",
    "            delta_wt[j], momentum = momentum_update(delta_wt[j], delta_wt[j], initial_learning_rate, beta, momentum)\n",
    "            # print(f\"wt{j}: \", delta_wt[j])\n",
    "        print ('Loss is: ', delta_wt)\n",
    "    return delta_wt\n",
    "\n",
    "\n",
    "\n",
    "adam(delta_wt, epsilon, 0.9, 0.999, initial_learning_rate)\n",
    "print(\"-------------------\")\n",
    "momentum(delta_wt, 0.9, initial_learning_rate, 10)\n",
    "print(\"-------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iitm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
