{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convoluntional Neural Network (CNN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, -1, 2, -2], [0, 0, 0, 0], [1, -1, 2, -2]])\n",
    "K = np.array([0.6, -0.2])\n",
    "stride = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8000000000000003\n"
     ]
    }
   ],
   "source": [
    "def calculate_1d_kernel(X, K, stride, padding) -> np.ndarray:\n",
    "    # Apply zero padding along axis 1 (columns)\n",
    "    X_padded = np.pad(X, ((0, 0), (padding, padding)), mode='constant', constant_values=0)\n",
    "\n",
    "    # Calculate output shape\n",
    "    output_shape_x = X_padded.shape[0]  # Number of rows remains the same\n",
    "    output_shape_y = (X_padded.shape[1] - K.shape[0]) // stride + 1\n",
    "    output_shape = (output_shape_x, output_shape_y)\n",
    "\n",
    "    # Initialize output matrix\n",
    "    output_mat = np.zeros(output_shape)\n",
    "\n",
    "    # Apply 1D convolution across each row\n",
    "    for i in range(output_shape_x):  # Iterate over rows\n",
    "        for j in range(output_shape_y):  # Iterate over columns\n",
    "            region = X_padded[i, j * stride: j * stride + K.shape[0]]  # Slice 1D region\n",
    "            output_mat[i, j] = np.sum(region * K)  # Convolve\n",
    "    \n",
    "    return output_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X) -> np.ndarray:\n",
    "    # calculate the sigmoid of the matrix X\n",
    "    return 1 / (1 + np.exp(-X))\n",
    "\n",
    "def flattner(X) -> np.ndarray:\n",
    "    # flatten the matrix X\n",
    "    return X.flatten()\n",
    "\n",
    "def mse_loss(y_true, y_pred) -> float:\n",
    "    # calculate the mean square error between y_true and y_pred\n",
    "    return 0.5*((y_true - y_pred) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.081868575263064\n",
      "0.9426758241011313\n",
      "0.35505127226226035\n"
     ]
    }
   ],
   "source": [
    "output = calculate_1d_kernel(X, K, stride, 0)\n",
    "print(sigmoid(output).sum())\n",
    "print(sigmoid(flattner(output).sum()))\n",
    "print(mse_loss(0.1, sigmoid(flattner(output).sum())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (9,) (2,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dOutput_dK\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Calculate the gradient\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m dK \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflattner\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(dK)\n",
      "Cell \u001b[1;32mIn[46], line 25\u001b[0m, in \u001b[0;36mcalculate_gradient\u001b[1;34m(X, K, stride, padding, y_true, y_pred)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m     24\u001b[0m         region \u001b[38;5;241m=\u001b[39m X[i, j \u001b[38;5;241m*\u001b[39m stride: j \u001b[38;5;241m*\u001b[39m stride \u001b[38;5;241m+\u001b[39m K\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m---> 25\u001b[0m         dOutput_dK \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mdL_dSigmoid\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdSigmoid_dOutput\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mregion\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dOutput_dK\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (9,) (2,) "
     ]
    }
   ],
   "source": [
    "# Now, do backpropagation to compute the gradients and update the weights w1=0.6 w1=0.6 and w2=-0.2 Assume Î·=1.\n",
    "\n",
    "def sigmoid_derivative(X) -> np.ndarray:\n",
    "    # calculate the derivative of the sigmoid of the matrix X\n",
    "    return sigmoid(X) * (1 - sigmoid(X))\n",
    "\n",
    "def calculate_gradient(X, K, stride, padding, y_true, y_pred) -> np.ndarray:\n",
    "    # Calculate the gradient of the loss with respect to the kernel K\n",
    "    output = calculate_1d_kernel(X, K, stride, padding)\n",
    "    output_flattened = flattner(output)\n",
    "    sigmoid_output = sigmoid(output_flattened)\n",
    "    loss = mse_loss(y_true, sigmoid_output)\n",
    "\n",
    "    # Calculate the gradient of the loss with respect to the sigmoid output\n",
    "    dL_dSigmoid = -(y_true - sigmoid_output)\n",
    "\n",
    "    # Calculate the gradient of the sigmoid output with respect to the output\n",
    "    dSigmoid_dOutput = sigmoid_derivative(output_flattened)\n",
    "\n",
    "    # Calculate the gradient of the output with respect to the kernel K\n",
    "    dOutput_dK = np.zeros(K.shape)\n",
    "    for i in range(output.shape[0]):\n",
    "        for j in range(output.shape[1]):\n",
    "            region = X[i, j * stride: j * stride + K.shape[0]]\n",
    "            dOutput_dK += dL_dSigmoid * dSigmoid_dOutput[i * output.shape[1] + j] * region\n",
    "\n",
    "    return dOutput_dK\n",
    "\n",
    "# Calculate the gradient\n",
    "dK = calculate_gradient(X, K, stride, 0, 0.1, sigmoid(flattner(output).sum()))\n",
    "print(dK)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iitm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
