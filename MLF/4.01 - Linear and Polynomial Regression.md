# Linear Regression

- Linear regression is a linear model, e.g. a model that assumes a linear relationship between the input variables (x) and the single output variable (y). More specifically, that y can be calculated from a linear combination of the input variables (x).
- Given Data $\{(x,y), \dots, (x_n,y_m)\}, x_i \in \mathbb{R}^d, y_i \in \mathbb{R}, i = 1, \dots, n$

  $$L(\theta) =  \frac{1}{2} \sum_{i=1}^n (y_i - \theta^T x_i)^2$$

- Minimize $L:$ Define
- $$A = \begin{bmatrix}x_1^T \\   x_2^T \\\vdots \\x_n^T \\\end{bmatrix}$$
- $$
  A\theta =  \begin{bmatrix}
   x_1^T \theta \\
   x_2^T \theta \\
    \vdots \\
   x_n^T \theta \\
  \end{bmatrix}
  $$
- $$
    A\theta - y = \begin{bmatrix}
     x_1^T \theta - y_1 \\
     x_2^T \theta - y_2 \\
      \vdots \\
     x_n^T \theta - y_n \\
    \end{bmatrix}
  $$
- $$(A\theta-y)^T(A\theta-y) = \sum_{i=1}^n (x_i^T\theta - y_i)^2$$
- So $L(\theta) = \frac{1}{2} (A\theta-y)^T(A\theta-y)$

- Minimizing L: $\nabla L(\theta) = 0 \rightarrow \theta = (A^TA)^{-1}A^Ty$

### Maximum Likelihood and Least Squares

- Suppose the data if generated according to the following model:
- $$y_i = \theta^T x_i + \epsilon_i$$
  - $\epsilon_i$ is a random variable with mean 0 and variance $\sigma^2$
  - $\epsilon_i$ is the error term or noise
- Dataset $D = \{x_i, y_i\}_{i=1}^n$ is generated iid according to the above model
- Maximum Likelihood approach:
  - $L(\theta) = \prod_{i=1}^n \frac{\sqrt{B}}{\sqrt{2\pi}} \cdot \exp\left(-\frac{1}{2}\frac{(y_i - \theta^T x_i)^2}{B}\right)$
- ML is equivalent to minimizing the sum of squared errors

### Likelihood Function

- Instead of maximizing the probability of the data, we can maximize the likelihood of the data $\log L(\theta)$
- Since $\log$ is a monotonic function, maximizing the likelihood is equivalent to maximizing the log likelihood
- $$\log L(\theta) = \frac{n}{2} \log \beta - \frac{n}{2} \log 2\pi - \beta \frac{1}{2} \sum_{i=1}^n (y_i - \theta^T x_i)^2$$
- Maximizing the log likelihood is equivalent to minimizing the sum of squared errors
- **The Least Squares approach is equivalent to the Maximum Likelihood approach**

# Polynomial Regression

- Consider one-dimensional data $\{(x_1, y_1), \dots, (x_n, y_n)\}$
- We want to fit a polynomial of degree $d$ to the data
- Transformed features:
  - $$
    \begin{align*}
    \hat{y}(x) & = \theta_0 + \theta_1 x + \theta_2 x^2 + \dots + \theta_d x^d \\ & = \sum_{i=0}^d \theta_i \cdot \phi_i(x) \end{align*}
    $$
    - $\phi_i(x) = x^i$
- Using these transformed features, we can use the same linear regression model
- $(A^TA)^\theta = A^Ty$
- This is different w.r.t. the original features, but it is linear w.r.t. the transformed features $\phi_i(x)$
- Regularized version of linear regression (a.k.a) **Ridge Regression**.
  - Instead of solving for $min_\theta \frac{1}{2} \sum_{i=1}^n (y_i - \theta^T x_i)^2$, we solve for the regularized version which is
    $$ \bar{\ L \ }(\theta) = \frac{1}{2} \sum\_{i=1}^n (x^T_i\theta - y_i)^2 + \lambda||\theta||^2$$
  - The regularization term is $\lambda||\theta||^2$
- If we repeat the calculation leading to the least squares solution, we get
  - $$\theta_{reg} (A^TA + \lambda I)^{-1} = A^Ty$$

