# Variance and Standard Deviation
## Variance
### Definition
- The variance of a random variable $X$ is denoted by $Var(X)$, and is defined as
  $$Var(X) = E[(X - E[X])^2]$$
  $$\text { or }$$
  $$Var(X) = E[X^2] - E[X]^2$$
- Variance is the expected value of the random variable $(X - E[X])^2$.
  - $Var(X)  = \sum_{t \in T_X}(t-E[X])^2 \cdot P(X=t)$
  - Variance is non-negative and standarad deviation is well defined.
  - More "spread out" the values of $X$ are, the larger the variance.
## Standard Deviation
### Definition
- The standard deviation of a random variable $X$ is denoted by $SD(X)$, and is defined as
  $$SD(X) = +\sqrt{Var(X)}$$
- Units of standard deviation are the same as the units of the random variable.

### Properties
- Let $X$ be random variable. Let $a$ be a constant real number.
  - $Var(aX) = a^2 Var(X)$
  - $SD(aX) = |a| SD(X)$
  - $Var(X + a) = Var(X)$
  - $SD(X + a) = SD(X)$

### Example
- $X \sim \text{ Uniform } \{1,2,3,4,5,6\}$
  - $E[X] = 3.5$
  - $Var(X) = E[(X - E[X])^2] = \sum_{t=1}^6 (t-3.5)^2 \cdot \frac{1}{6} = 2.9167...$
  - $SD(X) = \sqrt{Var(X)} = 1.7078...$
## Product and Sum of independent random variables
- Suppose $X$ and $Y$ are independent random variables. Then:
- $E[XY] = E[X] \cdot E[Y]$
- $Var(X + Y) = Var(X) + Var(Y)$

## Variance of Common Distributions
| Distribution | Expected Value | Variance |
| --- | --- | --- |
| Bernoulli(p) | $p$ | $p(1-p)$ |
| Binomial(n,p) | $np$ | $np(1-p)$ |
| Geometric(p) | $\frac{1}{p}$ | $\frac{1-p}{p^2}$ |
| Poisson($\lambda$) | $\lambda$ | $\lambda$ |
| Uniform(1,\dots,n) | $\frac{n+1}{2}$ | $\frac{n^2-1}{12}$ |
## Stardised Random Variable
### Defination
- A randon variable $X$ is said to be standardised if
$$E[X] = 0, Var(X) = 1$$
### Theorem
- Let $X$ be a random variable. Then,
$$ \frac{X-E[X]}{SD(X)}$$