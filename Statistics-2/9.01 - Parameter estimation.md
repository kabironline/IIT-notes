# Parameter Estimation
### Defination
- $i.i.d.$ Samples
- $X_1, X_2, \dots, X_n$ are $i.i.d. \ X$
- $X$ has a distribution described by some parameter $\theta_1,\theta_2,\dots$
  - Parameter takes real values
- Parameter estimation : estimate $\theta_1,\theta_2,\dots$ from $X_1, X_2, \dots, X_n$
- Estimator for a parameter $\theta$
  - Function of samples: $\hat{\theta} = g(X_1, X_2, \dots, X_n)$
  - Notation: $\hat{\theta}$ is an estimator the parameter $\theta$
- Parameter vs Estimator
  - $\theta:$ constant parameter, not a random value
  - $\hat{\theta}:$ funciton of $n$ random variable, therefore, it is a random variable
    - $\hat{\theta}$ is a random variable, it has a distribution n
### Example
- $X_1, X_2, \dots, X_n$ are $i.i.d. \ Binomial (p)$
- Parameter: $p$
  - $p$ is a constant
- Estimator $1: \hat{p_1}= 1/2$
- Estimator $2: \hat{p_2}= (X_1+X_2)/2$
- Estimator $3: \hat{p_3}= \bar{X}/n$
- There can be infinite number of estimators
  - How to choose a good estimator?
  - How to measure the quality of an estimator?