# Bias
$$X_1, X_2, \dots, X_n \sim X, \text{parameter } \theta$$
- Estimator for $\theta: \hat{\theta} = \theta(X_1, X_2, \dots, X_n)$ or $\hat{\theta}$ for short
### Defination
- The bias of the estimator $\hat{\theta}$ is defined as
   $$Bias(\hat{\theta}) = E[\hat{\theta}] - \theta$$
- Since Error = $\hat{\theta} - \theta$, bias is the expected value of the error
- An estimator with bias $0$ is called an unbiased estimator
# Risk
### Defination
- The (squared-error) risk of the estimator $\hat{\theta}$ for the parameter $\theta$ is defined as
   $$R(\hat{\theta}, \theta) = E[(\hat{\theta} - \theta)^2]$$
- Risk is the expected value of the squared error
- Since Error = $\hat{\theta} - \theta$, risk is the expected value of the squared error and is also called the mean squared error.
- Squared-error risk is the second moment of Error.

# Variance
- Variance of an estimator is the risk of the estimator when the parameter is equal to the expected value of the estimator
  $$Var(\hat{\theta}) = E[(\hat{\theta}- E[\hat\theta])^2]$$
  - Variance of error: Error "translated" version of the estimator $\hat\theta$
  - $\theta$ is a constant, so $E[\theta] = \theta$
  $$Var(\text{Error}) = Var(\hat{\theta})$$
# Bias Variance Tradeoff
- The risk of the estimator satisfies the following relationship:
  $$Risk(\hat{\theta}, \theta) = Bias(\hat{\theta}), \theta)^2 + Var(\hat{\theta})$$
- Expanded form:
  $$ E[(\hat{\theta} - \theta)^2] = (E[\hat{\theta}] - \theta)^2 + E[(\hat{\theta} - E[\hat{\theta}])^2]$$
- Proof
  $$ Risk = E[\text{Error}^2] = \text{Mean[Error}]^2 + Var(\text{Error})$$
