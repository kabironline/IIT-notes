# Likelihood of $i.i.d.$ samples
- $X_1, X_2, \dots, X_n \sim f_X(x; \theta, \theta_2, \dots, \theta_k)$
- $f_X(x): depends on \theta, \theta_2, \dots, \theta_k$
  - To bring this out, we will write $f_X(x)$ as $f_X(x; \theta, \theta_2, \dots, \theta_k)$
- Likelihood of a sampling $x_1, x_2, \dots, x_n$ is defined as
  $$L(\theta, \theta_2, \dots, \theta_k; x_1, x_2, \dots, x_n) = \prod_{i=1}^n f_X(x_i; \theta, \theta_2, \dots, \theta_k)$$
### Example
- $Bernoulli(p): 1,0,0,1,0,1,1,1,0,0$
  - $L = p(1-p)(1-p)p(1-p)ppp(1-p)(1-p) = p^5(1-p)^5$
# Maximum Likelihood Estimation
$$X_1, \dots, X_n \sim i.i.d. X, \text{parameter } \theta_1, \theta_2, \dots, \theta_n$$
- Likelihood $L(x_1, \dots, x_n)$ is a function of paramters
- Maximum likelihood (ML) estimation
  - Sampling: $x_1, \dots, x_n$
   $$\theta_1^*, \theta_2^*, \dots = \arg\max_{\theta_1, \theta_2, \dots} L(\theta_1, \theta_2, \dots; x_1, x_2, \dots, x_n)$$
- Find parameters that maximize that maximize likelihood for a given set of samples.
- When the maximization problem has a closed-form solution, the estimators can be expressed in terms of the samples.
  -  This will require a lot of algebraic manipulation.
- In many cases, the maximization problem will need a numberical routine to solve.
  - Several standard numerical routines are available in Matlab, R, Python, etc.
### Example
- $Bernoulli(p)$
$$X_1, \dots, X_n \sim Bernoulli(p)$$
- Samples $x_1, \dots, x_n$
  - $x_i = 0 \text{ or } 1$
- Likelihood: $L(x_1, \dots, x_n) = \prod_{i=1}^n f_X(x_i)$
  - $f_X(x_i) = p$ if $x_i = 1$ or $f_X(x_i) = 1-p$ if $x_i = 0$
  - Let $w$ denote the number of 1's in the sample
    $$L(x_1, \dots, x_n) = p^w(1-p)^{n-w}$$
- ML estimation
  - Differentiate $L$ and equate to $0$
  - We can also maximize L by maximizing $\log L$
    - $\log L = w\log p + (n-w)\log(1-p)$
    - Differentiate $\log L$ and equate to $0$
    - $\frac{dh(p)}{dp} = \frac{w}{p} - \frac{n-w}{1-p} = 0$
    - $w(1-p) = (n-w)p$
    - $p = \frac{w}{n}$
    - $or$
    - $p = \frac{1}{n}\$
---
- $Poission(\lambda)$
$$X_1, \dots, X_n \sim Poission(\lambda)$$
- Likelihood: $L(x_1,\dots,x_n) = \prod_{i=1}^ne^{-\lambda}\frac{\lambda^{x_i}}{x_i}$
- $L(x_1,\dots,x_n) = \frac{1}{x_1!x_2!\dots x_n!}e^{-n\lambda}\lambda^{x_1+\dots+x_n}$
- ML estimation of $\lambda$
  - $\log L = (x_1 + \dots + x_n) \log(\lambda- n\lambda)$
    - $\lambda^* = \frac{x_1 + \dots + x_n}{n}$
---
- $Normal(\mu, \sigma^2)$
$$X_1, \dots, X_n \sim Normal(\mu, \sigma^2)$$
- Likelihood: $L(x_1, \dots, x_n) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x_i-\mu)^2}{2\sigma^2}}$
  - $L(x_1, \dots, x_n) = (\frac{1}{2\pi\sigma})^ne^{\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2}$
  - Taking Log
  - ML estimation: $\mu^*, \sigma^{*} = \arg\max[\frac{1}{2\sigma^2}\sum(x_i-\mu)^2+n \log \sigma]$
  - $\hat{\mu}_{ML} = \frac{X_1 + \dots + X_n}{n}$
    - Differentiate $w.r.t. \mu \text{(treat )} \sigma \text{ as constant}$
  - $\hat{\sigma}_{ML}^2 = \frac{1}{n}(X_1 - \hat{\mu}_{ML})^2 + \dots + (X_n - \hat{\mu}_{ML})^2$
