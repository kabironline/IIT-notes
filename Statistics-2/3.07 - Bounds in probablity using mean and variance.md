# Bounds in probability using mean and variance
## Markov's inequality
### Theorem
- Let $X$ be a discrete random variable that takes only non-negative values with a finite mean $\mu$. Then:
$$ P(X \geq c) \leq \frac{\mu}{c} $$

## Chebyshev's inequality
- Let $X$ be a random variable with mean $\mu$ with a finite mean $\mu$ and variance $\sigma^2$. Then:
  $$ P(|X - \mu| \geq k\sigma) \leq \frac{1}{k^2} $$
- Other forms:
  $$ P(|X - \mu| \geq c) \leq \frac{\sigma^2}{c^2} , P((X - \mu)^2 \geq k^2\sigma^2) $$
  $$ P(C \geq \mu + k\sigma) < X < \mu + k\sigma) \geq 1 - \frac{1}{k^2} $$
  $$ P (X\geq \mu + k\sigma) + P(X \leq \mu - k\sigma) \leq \frac{1}{k^2} $$